{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54888448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/09 09:30:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.6:55148) with ID 0\n",
      "23/02/09 09:30:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.23.0.7:60686) with ID 1\n",
      "23/02/09 09:30:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.7:37293 with 127.2 MiB RAM, BlockManagerId(1, 172.23.0.7, 37293, None)\n",
      "23/02/09 09:30:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.23.0.6:34101 with 127.2 MiB RAM, BlockManagerId(0, 172.23.0.6, 34101, None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = '/opt/.venv/bin/python'\n",
    "#os.environ[\"SPARK_HOME\"] = '/opt/spark'\n",
    "\n",
    "def init_spark():\n",
    "    \"\"\"\n",
    "    Initializes SparkSession and returns SparkSession and SparkContext objects\n",
    "    \n",
    "    Returns:\n",
    "        SparkSession, SparkContext: objects of SparkSession and SparkContext respectively\n",
    "    \"\"\"\n",
    "    logging.info(\"Initializing Spark session\")\n",
    "    spark = SparkSession.builder\\\n",
    "                        .appName(\"query\")\\\n",
    "                        .master(\"spark://spark-master:7077\")\\\n",
    "                        .config(\"spark.jars\", \"/opt/workspace/spark-apps/packages/postgresql-42.2.22.jar,/opt/workspace/spark-apps/packages/spark-avro_2.12-3.0.2.jar\")\\\n",
    "                        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "                        .getOrCreate()\n",
    "    logging.info(\"Spark session initialized successfully\")\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16437612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|1,Product Management|\n",
      "|             2,Sales|\n",
      "|3,Research and De...|\n",
      "|4,Business Develo...|\n",
      "|       5,Engineering|\n",
      "|   6,Human Resources|\n",
      "|          7,Services|\n",
      "|           8,Support|\n",
      "|         9,Marketing|\n",
      "|         10,Training|\n",
      "|            11,Legal|\n",
      "|       12,Accounting|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.load('/opt/workspace/data/historic/departments/*.csv', format=\"csv\", inferSchema=\"true\", sep=\"\\t\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff24dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          department|\n",
      "+---+--------------------+\n",
      "|  1|  Product Management|\n",
      "|  2|               Sales|\n",
      "|  3|Research and Deve...|\n",
      "|  4|Business Development|\n",
      "|  5|         Engineering|\n",
      "|  6|     Human Resources|\n",
      "|  7|            Services|\n",
      "|  8|             Support|\n",
      "|  9|           Marketing|\n",
      "| 10|            Training|\n",
      "| 11|               Legal|\n",
      "| 12|          Accounting|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"department\", StringType(), True)\n",
    "          ])\n",
    "\n",
    "df = spark.read.load('/opt/workspace/data/historic/departments/*.csv', format=\"csv\", schema=schema, sep=\",\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b83f318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891dd4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          department|\n",
      "+---+--------------------+\n",
      "|  1|  Product Management|\n",
      "|  2|               Sales|\n",
      "|  3|Research and Deve...|\n",
      "|  4|Business Development|\n",
      "|  5|         Engineering|\n",
      "|  6|     Human Resources|\n",
      "|  7|            Services|\n",
      "|  8|             Support|\n",
      "|  9|           Marketing|\n",
      "| 10|            Training|\n",
      "| 11|               Legal|\n",
      "| 12|          Accounting|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b88053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "url = \"jdbc:postgresql://postgres_database:5432/globant_challenge_db\"\n",
    "properties = {\n",
    "    \"user\": \"globant_super_admin\",\n",
    "    \"password\": \"pass1234\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df = df.drop('id')\n",
    "\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"dbo.departments\") \\\n",
    "    .option(\"user\", \"globant_super_admin\") \\\n",
    "    .option(\"password\", \"pass1234\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e14f54",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "options() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbo.department\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moptions(mode\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: options() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "df.write.format('jdbc').options('url', url).options('table', 'dbo.department').options(mode+'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f8ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://postgres_database:5432/globant_challenge_db\"\n",
    "properties = {\n",
    "    \"user\": \"globant_super_admin\",\n",
    "    \"password\": \"pass1234\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", url) \\\n",
    "                .option(\"driver\", properties[\"driver\"]) \\\n",
    "                .option(\"dbtable\", f\"dbo.departments\") \\\n",
    "                .option(\"user\", properties[\"user\"]) \\\n",
    "                .option(\"password\", properties[\"password\"]) \\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b49826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/09 09:31:14 INFO CodeGenerator: Code generated in 828.841606 ms\n",
      "23/02/09 09:31:14 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/02/09 09:31:14 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/02/09 09:31:14 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/02/09 09:31:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/02/09 09:31:14 INFO DAGScheduler: Missing parents: List()\n",
      "23/02/09 09:31:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/02/09 09:31:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)\n",
      "23/02/09 09:31:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.4 MiB)\n",
      "23/02/09 09:31:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f7a220ef6852:35751 (size: 4.8 KiB, free: 434.4 MiB)\n",
      "23/02/09 09:31:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223\n",
      "23/02/09 09:31:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/02/09 09:31:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "23/02/09 09:31:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.23.0.6, executor 0, partition 0, PROCESS_LOCAL, 7175 bytes)\n",
      "23/02/09 09:31:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.23.0.6:34101 (size: 4.8 KiB, free: 127.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          department|\n",
      "+---+--------------------+\n",
      "|  1|  Product Management|\n",
      "|  2|               Sales|\n",
      "|  3|Research and Deve...|\n",
      "|  4|Business Development|\n",
      "|  5|         Engineering|\n",
      "|  6|     Human Resources|\n",
      "|  7|            Services|\n",
      "|  8|             Support|\n",
      "|  9|           Marketing|\n",
      "| 10|            Training|\n",
      "| 11|               Legal|\n",
      "| 12|          Accounting|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/09 09:31:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3361 ms on 172.23.0.6 (executor 0) (1/1)\n",
      "23/02/09 09:31:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/02/09 09:31:19 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 4.057 s\n",
      "23/02/09 09:31:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/02/09 09:31:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/02/09 09:31:19 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 4.131217 s\n",
      "23/02/09 09:31:19 INFO CodeGenerator: Code generated in 21.908301 ms            \n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89551410",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/workspace/data/backups/departments/departments.avro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py:830\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/09 09:36:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f7a220ef6852:35751 in memory (size: 4.8 KiB, free: 434.4 MiB)\n",
      "23/02/09 09:36:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.23.0.6:34101 in memory (size: 4.8 KiB, free: 127.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"avro\").save(\"/opt/workspace/data/backups/departments/departments.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8229f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
